{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506578d5-808e-4508-8bb3-aac00f8d7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cab621e-bc5f-464e-90d9-d1d0761b77c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bc6888bcf6fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Request 'http://127.0.0.1:5001/act' [POST]>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Update assignee for the issue.\n",
      "Issue ID: 2, Assignee: yasonk\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  update_issue (call_peJB6v6SPB7kZeahowk35yM7)\n",
      " Call ID: call_peJB6v6SPB7kZeahowk35yM7\n",
      "  Args:\n",
      "    assignees: ['yasonk']\n",
      "    issue_number: 2\n",
      "    owner: breba-apps\n",
      "    repo: TempRepo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: update_issue\n",
      "\n",
      "Error: McpError('failed to update issue: PATCH https://api.github.com/repos/breba-apps/TempRepo/issues/2: 403 Resource not accessible by personal access token []')\n",
      " Please fix your mistakes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/May/2025 22:30:18] \"POST /act HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The issue could be due to insufficient permissions with the current access token. Please ensure that the token has the necessary scope to update issues.\n"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "from pathlib import Path\n",
    "from flask import Flask, request\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Define the path to your MCP server\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"-i\",\n",
    "        \"--rm\",\n",
    "        \"-e\",\n",
    "        \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n",
    "        \"ghcr.io/github/github-mcp-server\"\n",
    "      ],\n",
    "    env={\n",
    "        \"GITHUB_PERSONAL_ACCESS_TOKEN\": os.environ.get(\"GITHUB_PERSONAL_ACCESS_TOKEN\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "@asynccontextmanager\n",
    "async def tool_session_generator():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await load_mcp_tools(session)\n",
    "            yield tools\n",
    "\n",
    "\n",
    "async def ai(query: str):\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    async with tool_session_generator() as tools:\n",
    "        agent_executor = create_react_agent(llm, tools)\n",
    "    \n",
    "        # System prompt allows for broad range of instructions. But what does user input look like?\n",
    "        events = agent_executor.astream(\n",
    "            {\"messages\": [(\"system\", Path(\"web_server_context.txt\").read_text()),\n",
    "                          (\"user\", query)]},\n",
    "            stream_mode=\"values\",\n",
    "        )\n",
    "    \n",
    "        async for event in events:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "            data = event[\"messages\"][-1]\n",
    "    \n",
    "        return data.content\n",
    "\n",
    "\n",
    "async def ai_act(query: str):\n",
    "    act_llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "    async with tool_session_generator() as tools:\n",
    "        act_agent = create_react_agent(act_llm, tools)\n",
    "    \n",
    "        events = act_agent.astream(\n",
    "            {\"messages\": [(\"system\",\n",
    "                           \"You are a web server. Do not use markdown for formatting. Do exactly what the user asks. Do not use markdown in your response. Produce minimal output. Given my repo: https://github.com/breba-apps/TempRepo\"),\n",
    "                          (\"user\", query)]},\n",
    "            stream_mode=\"values\",\n",
    "        )\n",
    "        data = \"\"\n",
    "        async for event in events:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "            data = event[\"messages\"][-1]\n",
    "    \n",
    "        return data.content\n",
    "        \n",
    "\n",
    "app = Flask(__name__)\n",
    "    \n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_data():\n",
    "    query = Path(\"my_app-step-4-mcp.txt\").read_text()\n",
    "    response = asyncio.run(ai(query))\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.route('/act', methods=['POST'])\n",
    "def act():\n",
    "    payload = request.data.decode('utf-8')\n",
    "    print(request)\n",
    "    response = asyncio.run(ai_act(payload))\n",
    "    return response\n",
    "\n",
    "\n",
    "app.run(debug=False, port=5001, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a9b5a-31f5-4994-9a44-ba6dae6da12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083002a4-d5e8-4db3-bf46-8e83c9904557",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Write natural language only\n",
    "- Unstructured data\n",
    "- Implicit algorithms\n",
    "- ~~Dynamic tools~~\n",
    "- ~~Human in the loop built-in~~\n",
    "- ~~Consistent output~~\n",
    "- ~~Iterative development~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e882fe-f722-432c-87f4-cfb0258f6a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
