{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506578d5-808e-4508-8bb3-aac00f8d7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab621e-bc5f-464e-90d9-d1d0761b77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506bc6888bcf6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "from pathlib import Path\n",
    "from flask import Flask, request\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Define the path to your MCP server\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"docker\",\n",
    "    args=[\n",
    "        \"run\",\n",
    "        \"-i\",\n",
    "        \"--rm\",\n",
    "        \"-e\",\n",
    "        \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n",
    "        \"ghcr.io/github/github-mcp-server\"\n",
    "      ],\n",
    "    env={\n",
    "        \"GITHUB_PERSONAL_ACCESS_TOKEN\": os.environ.get(\"GITHUB_PERSONAL_ACCESS_TOKEN\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "@asynccontextmanager\n",
    "async def github_mcp():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await load_mcp_tools(session)\n",
    "            yield tools\n",
    "\n",
    "\n",
    "async def ai(query: str):\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    async with github_mcp() as tools:\n",
    "        agent_executor = create_react_agent(llm, tools)\n",
    "    \n",
    "        # System prompt allows for broad range of instructions. But what does user input look like?\n",
    "        events = agent_executor.astream(\n",
    "            {\"messages\": [(\"system\", Path(\"web_server_context.txt\").read_text()),\n",
    "                          (\"user\", query)]},\n",
    "            stream_mode=\"values\",\n",
    "        )\n",
    "    \n",
    "        async for event in events:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "            data = event[\"messages\"][-1]\n",
    "    \n",
    "        return data.content\n",
    "\n",
    "\n",
    "async def ai_act(query: str):\n",
    "    act_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "    async with github_mcp() as tools:\n",
    "        act_agent = create_react_agent(act_llm, tools)\n",
    "    \n",
    "        events = act_agent.astream(\n",
    "            {\"messages\": [(\"system\",\n",
    "                           \"You are a web server. Do not use markdown for formatting. Do exactly what the user asks. Do not use markdown in your response. Produce minimal output. Given my repo: https://github.com/breba-apps/TempRepo\"),\n",
    "                          (\"user\", query)]},\n",
    "            stream_mode=\"values\",\n",
    "        )\n",
    "        data = \"\"\n",
    "        async for event in events:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "            data = event[\"messages\"][-1]\n",
    "    \n",
    "        return data.content\n",
    "        \n",
    "\n",
    "app = Flask(__name__)\n",
    "    \n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_data():\n",
    "    query = Path(\"my_app-step-4-mcp.txt\").read_text()\n",
    "    response = asyncio.run(ai(query))\n",
    "    return response\n",
    "\n",
    "\n",
    "@app.route('/act', methods=['POST'])\n",
    "def act():\n",
    "    payload = request.data.decode('utf-8')\n",
    "    print(request)\n",
    "    response = asyncio.run(ai_act(payload))\n",
    "    return response\n",
    "\n",
    "\n",
    "app.run(debug=False, port=5001, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a9b5a-31f5-4994-9a44-ba6dae6da12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083002a4-d5e8-4db3-bf46-8e83c9904557",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Write natural language only\n",
    "- Unstructured data\n",
    "- Implicit algorithms\n",
    "- ~~Dynamic tools~~\n",
    "- ~~Human in the loop built-in~~\n",
    "- ~~Consistent output~~\n",
    "- ~~Iterative development~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e882fe-f722-432c-87f4-cfb0258f6a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
